{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to enable loss function to be flexibly used for\n",
    "# both 2D or 3D image segmentation - source: https://github.com/frankkramer-lab/MIScnn\n",
    "def identify_axis(shape):\n",
    "    # Three dimensional\n",
    "    if len(shape) == 5:\n",
    "        return [1, 2, 3]\n",
    "    # Two dimensional\n",
    "    elif len(shape) == 4:\n",
    "        return [1, 2]\n",
    "    # Exception - Unknown\n",
    "    else:\n",
    "        raise ValueError(\"Metric: Shape of tensor is neither 2D or 3D.\")\n",
    "\n",
    "\n",
    "################################\n",
    "#           Dice loss          #\n",
    "################################\n",
    "def dice_loss(delta=0.5, smooth=0.000001):\n",
    "    \"\"\"Dice loss originates from Sørensen–Dice coefficient, which is a statistic developed in 1940s to gauge the similarity between two samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float, optional\n",
    "        controls weight given to false positive and false negatives, by default 0.5\n",
    "    smooth : float, optional\n",
    "        smoothing constant to prevent division by zero errors, by default 0.000001\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        # Calculate true positives (tp), false negatives (fn) and false positives (fp)\n",
    "        tp = K.sum(y_true * y_pred, axis=axis)\n",
    "        fn = K.sum(y_true * (1 - y_pred), axis=axis)\n",
    "        fp = K.sum((1 - y_true) * y_pred, axis=axis)\n",
    "        # Calculate Dice score\n",
    "        dice_class = (tp + smooth) / (tp + delta * fn + (1 - delta) * fp + smooth)\n",
    "        # Average class scores\n",
    "        dice_loss = K.mean(1 - dice_class)\n",
    "\n",
    "        return dice_loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "################################\n",
    "#         Tversky loss         #\n",
    "################################\n",
    "def tversky_loss(delta=0.7, smooth=0.000001):\n",
    "    \"\"\"Tversky loss function for image segmentation using 3D fully convolutional deep networks\n",
    "        Link: https://arxiv.org/abs/1706.05721\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float, optional\n",
    "        controls weight given to false positive and false negatives, by default 0.7\n",
    "    smooth : float, optional\n",
    "        smoothing constant to prevent division by zero errors, by default 0.000001\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        # Calculate true positives (tp), false negatives (fn) and false positives (fp)\n",
    "        tp = K.sum(y_true * y_pred, axis=axis)\n",
    "        fn = K.sum(y_true * (1 - y_pred), axis=axis)\n",
    "        fp = K.sum((1 - y_true) * y_pred, axis=axis)\n",
    "        tversky_class = (tp + smooth) / (tp + delta * fn + (1 - delta) * fp + smooth)\n",
    "        # Average class scores\n",
    "        tversky_loss = K.mean(1 - tversky_class)\n",
    "\n",
    "        return tversky_loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "################################\n",
    "#       Dice coefficient       #\n",
    "################################\n",
    "def dice_coefficient(delta=0.5, smooth=0.000001):\n",
    "    \"\"\"The Dice similarity coefficient, also known as the Sørensen–Dice index or simply Dice coefficient, is a statistical tool which measures the similarity between two sets of data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float, optional\n",
    "        controls weight given to false positive and false negatives, by default 0.5\n",
    "    smooth : float, optional\n",
    "        smoothing constant to prevent division by zero errors, by default 0.000001\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        # Calculate true positives (tp), false negatives (fn) and false positives (fp)\n",
    "        tp = K.sum(y_true * y_pred, axis=axis)\n",
    "        fn = K.sum(y_true * (1 - y_pred), axis=axis)\n",
    "        fp = K.sum((1 - y_true) * y_pred, axis=axis)\n",
    "        dice_class = (tp + smooth) / (tp + delta * fn + (1 - delta) * fp + smooth)\n",
    "        # Average class scores\n",
    "        dice = K.mean(dice_class)\n",
    "\n",
    "        return dice\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "################################\n",
    "#          Combo loss          #\n",
    "################################\n",
    "def combo_loss(alpha=0.5, beta=0.5):\n",
    "    \"\"\"Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation\n",
    "    Link: https://arxiv.org/abs/1805.02798\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, optional\n",
    "        controls weighting of dice and cross-entropy loss., by default 0.5\n",
    "    beta : float, optional\n",
    "        beta > 0.5 penalises false negatives more than false positives., by default 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        dice = dice_coefficient()(y_true, y_pred)\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        # Clip values to prevent division by zero error\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        if beta is not None:\n",
    "            beta_weight = np.array([beta, 1 - beta])\n",
    "            cross_entropy = beta_weight * cross_entropy\n",
    "        # sum over classes\n",
    "        cross_entropy = K.mean(K.sum(cross_entropy, axis=[-1]))\n",
    "        if alpha is not None:\n",
    "            combo_loss = (alpha * cross_entropy) - ((1 - alpha) * dice)\n",
    "        else:\n",
    "            combo_loss = cross_entropy - dice\n",
    "        return combo_loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "################################\n",
    "#      Focal Tversky loss      #\n",
    "################################\n",
    "def focal_tversky_loss(delta=0.7, gamma=0.75, smooth=0.000001):\n",
    "    \"\"\"A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation\n",
    "    Link: https://arxiv.org/abs/1810.07842\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float, optional\n",
    "        focal parameter controls degree of down-weighting of easy examples, by default 0.75\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        # Clip values to prevent division by zero error\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        # Calculate true positives (tp), false negatives (fn) and false positives (fp)\n",
    "        tp = K.sum(y_true * y_pred, axis=axis)\n",
    "        fn = K.sum(y_true * (1 - y_pred), axis=axis)\n",
    "        fp = K.sum((1 - y_true) * y_pred, axis=axis)\n",
    "        tversky_class = (tp + smooth) / (tp + delta * fn + (1 - delta) * fp + smooth)\n",
    "        # Average class scores\n",
    "        focal_tversky_loss = K.mean(K.pow((1 - tversky_class), gamma))\n",
    "\n",
    "        return focal_tversky_loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "################################\n",
    "#          Focal loss          #\n",
    "################################\n",
    "def focal_loss(alpha=None, gamma_f=2.0):\n",
    "    \"\"\"Focal loss is used to address the issue of the class imbalance problem. A modulation term applied to the Cross-Entropy loss function.\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, optional\n",
    "        controls relative weight of false positives and false negatives. alpha > 0.5 penalises false negatives more than false positives, by default None\n",
    "    gamma_f : float, optional\n",
    "        focal parameter controls degree of down-weighting of easy examples, by default 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        # Clip values to prevent division by zero error\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        if alpha is not None:\n",
    "            alpha_weight = np.array(alpha, dtype=np.float32)\n",
    "            focal_loss = alpha_weight * K.pow(1 - y_pred, gamma_f) * cross_entropy\n",
    "        else:\n",
    "            focal_loss = K.pow(1 - y_pred, gamma_f) * cross_entropy\n",
    "\n",
    "        focal_loss = K.mean(K.sum(focal_loss, axis=[-1]))\n",
    "        return focal_loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "################################\n",
    "#       Symmetric Focal loss      #\n",
    "################################\n",
    "def symmetric_focal_loss(delta=0.7, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float, optional\n",
    "        controls weight given to false positive and false negatives, by default 0.7\n",
    "    gamma : float, optional\n",
    "        Focal Tversky loss' focal parameter controls degree of down-weighting of easy examples, by default 2.0\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        # calculate losses separately for each class\n",
    "        back_ce = K.pow(1 - y_pred[:, :, :, 0], gamma) * cross_entropy[:, :, :, 0]\n",
    "        back_ce = (1 - delta) * back_ce\n",
    "\n",
    "        fore_ce = K.pow(1 - y_pred[:, :, :, 1], gamma) * cross_entropy[:, :, :, 1]\n",
    "        fore_ce = delta * fore_ce\n",
    "\n",
    "        loss = K.mean(K.sum(tf.stack([back_ce, fore_ce], axis=-1), axis=-1))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "#################################\n",
    "# Symmetric Focal Tversky loss  #\n",
    "#################################\n",
    "def symmetric_focal_tversky_loss(delta=0.7, gamma=0.75):\n",
    "    \"\"\"This is the implementation for binary segmentation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float, optional\n",
    "        controls weight given to false positive and false negatives, by default 0.7\n",
    "    gamma : float, optional\n",
    "        focal parameter controls degree of down-weighting of easy examples, by default 0.75\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        # Clip values to prevent division by zero error\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        # Calculate true positives (tp), false negatives (fn) and false positives (fp)\n",
    "        tp = K.sum(y_true * y_pred, axis=axis)\n",
    "        fn = K.sum(y_true * (1 - y_pred), axis=axis)\n",
    "        fp = K.sum((1 - y_true) * y_pred, axis=axis)\n",
    "        dice_class = (tp + epsilon) / (tp + delta * fn + (1 - delta) * fp + epsilon)\n",
    "\n",
    "        # calculate losses separately for each class, enhancing both classes\n",
    "        back_dice = (1 - dice_class[:, 0]) * K.pow(1 - dice_class[:, 0], -gamma)\n",
    "        fore_dice = (1 - dice_class[:, 1]) * K.pow(1 - dice_class[:, 1], -gamma)\n",
    "\n",
    "        # Average class scores\n",
    "        loss = K.mean(tf.stack([back_dice, fore_dice], axis=-1))\n",
    "        return loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "################################\n",
    "#     Asymmetric Focal loss    #\n",
    "################################\n",
    "def asymmetric_focal_loss(delta=0.7, gamma=2.0):\n",
    "    \"\"\"For Imbalanced datasets\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float, optional\n",
    "        controls weight given to false positive and false negatives, by default 0.7\n",
    "    gamma : float, optional\n",
    "        Focal Tversky loss' focal parameter controls degree of down-weighting of easy examples, by default 2.0\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        # calculate losses separately for each class, only suppressing background class\n",
    "        back_ce = K.pow(1 - y_pred[:, :, :, 0], gamma) * cross_entropy[:, :, :, 0]\n",
    "        back_ce = (1 - delta) * back_ce\n",
    "        fore_ce = cross_entropy[:, :, :, 1]\n",
    "        fore_ce = delta * fore_ce\n",
    "        loss = K.mean(K.sum(tf.stack([back_ce, fore_ce], axis=-1), axis=-1))\n",
    "        return loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "#################################\n",
    "# Asymmetric Focal Tversky loss #\n",
    "#################################\n",
    "def asymmetric_focal_tversky_loss(delta=0.7, gamma=0.75):\n",
    "    \"\"\"This is the implementation for binary segmentation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float, optional\n",
    "        controls weight given to false positive and false negatives, by default 0.7\n",
    "    gamma : float, optional\n",
    "        focal parameter controls degree of down-weighting of easy examples, by default 0.75\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        # Clip values to prevent division by zero error\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "\n",
    "        axis = identify_axis(y_true.get_shape())\n",
    "        # Calculate true positives (tp), false negatives (fn) and false positives (fp)\n",
    "        tp = K.sum(y_true * y_pred, axis=axis)\n",
    "        fn = K.sum(y_true * (1 - y_pred), axis=axis)\n",
    "        fp = K.sum((1 - y_true) * y_pred, axis=axis)\n",
    "        dice_class = (tp + epsilon) / (tp + delta * fn + (1 - delta) * fp + epsilon)\n",
    "\n",
    "        # calculate losses separately for each class, only enhancing foreground class\n",
    "        back_dice = 1 - dice_class[:, 0]\n",
    "        fore_dice = (1 - dice_class[:, 1]) * K.pow(1 - dice_class[:, 1], -gamma)\n",
    "\n",
    "        # Average class scores\n",
    "        loss = K.mean(tf.stack([back_dice, fore_dice], axis=-1))\n",
    "        return loss\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "###########################################\n",
    "#      Symmetric Unified Focal loss       #\n",
    "###########################################\n",
    "def sym_unified_focal_loss(weight=0.5, delta=0.6, gamma=0.5):\n",
    "    \"\"\"The Unified Focal loss is a new compound loss function that unifies Dice-based and cross entropy-based loss functions into a single framework.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weight : float, optional\n",
    "        represents lambda parameter and controls weight given to symmetric Focal Tversky loss and symmetric Focal loss, by default 0.5\n",
    "    delta : float, optional\n",
    "        controls weight given to each class, by default 0.6\n",
    "    gamma : float, optional\n",
    "        focal parameter controls the degree of background suppression and foreground enhancement, by default 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        symmetric_ftl = symmetric_focal_tversky_loss(delta=delta, gamma=gamma)(\n",
    "            y_true, y_pred\n",
    "        )\n",
    "        symmetric_fl = symmetric_focal_loss(delta=delta, gamma=gamma)(y_true, y_pred)\n",
    "        if weight is not None:\n",
    "            return (weight * symmetric_ftl) + ((1 - weight) * symmetric_fl)\n",
    "        else:\n",
    "            return symmetric_ftl + symmetric_fl\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "\n",
    "###########################################\n",
    "#      Asymmetric Unified Focal loss      #\n",
    "###########################################\n",
    "def asym_unified_focal_loss(weight=0.5, delta=0.6, gamma=0.5):\n",
    "    \"\"\"The Unified Focal loss is a new compound loss function that unifies Dice-based and cross entropy-based loss functions into a single framework.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weight : float, optional\n",
    "        represents lambda parameter and controls weight given to asymmetric Focal Tversky loss and asymmetric Focal loss, by default 0.5\n",
    "    delta : float, optional\n",
    "        controls weight given to each class, by default 0.6\n",
    "    gamma : float, optional\n",
    "        focal parameter controls the degree of background suppression and foreground enhancement, by default 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "        asymmetric_ftl = asymmetric_focal_tversky_loss(delta=delta, gamma=gamma)(\n",
    "            y_true, y_pred\n",
    "        )\n",
    "        asymmetric_fl = asymmetric_focal_loss(delta=delta, gamma=gamma)(y_true, y_pred)\n",
    "        if weight is not None:\n",
    "            return (weight * asymmetric_ftl) + ((1 - weight) * asymmetric_fl)\n",
    "        else:\n",
    "            return asymmetric_ftl + asymmetric_fl\n",
    "\n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    return tn / (tn + fp + K.epsilon())\n",
    "\n",
    "\n",
    "def negative_predictive_value(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "    return tn / (tn + fn + K.epsilon())\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    num = (1 + beta**2) * (p * r)\n",
    "    den = beta**2 * p + r + K.epsilon()\n",
    "    return K.mean(num / den)\n",
    "\n",
    "\n",
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())\n",
    "\n",
    "\n",
    "def equal_error_rate(y_true, y_pred):\n",
    "    n_imp = tf.count_nonzero(tf.equal(y_true, 0), dtype=tf.float32) + tf.constant(\n",
    "        K.epsilon()\n",
    "    )\n",
    "    n_gen = tf.count_nonzero(tf.equal(y_true, 1), dtype=tf.float32) + tf.constant(\n",
    "        K.epsilon()\n",
    "    )\n",
    "\n",
    "    scores_imp = tf.boolean_mask(y_pred, tf.equal(y_true, 0))\n",
    "    scores_gen = tf.boolean_mask(y_pred, tf.equal(y_true, 1))\n",
    "\n",
    "    loop_vars = (tf.constant(0.0), tf.constant(1.0), tf.constant(0.0))\n",
    "    cond = lambda t, fpr, fnr: tf.greater_equal(fpr, fnr)\n",
    "    body = lambda t, fpr, fnr: (\n",
    "        t + 0.001,\n",
    "        tf.divide(\n",
    "            tf.count_nonzero(tf.greater_equal(scores_imp, t), dtype=tf.float32), n_imp\n",
    "        ),\n",
    "        tf.divide(tf.count_nonzero(tf.less(scores_gen, t), dtype=tf.float32), n_gen),\n",
    "    )\n",
    "    t, fpr, fnr = tf.while_loop(cond, body, loop_vars, back_prop=False)\n",
    "    eer = (fpr + fnr) / 2\n",
    "\n",
    "    return eer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
